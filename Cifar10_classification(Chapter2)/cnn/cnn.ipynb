{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll walk through the steps required to train your own convolutional neural network (CNN) on the CIFAR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "import lightning.pytorch as L\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "NUM_CLASSES = len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the Data <a name=\"prepare\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        # torchvision.transforms.RandomCrop(32, padding=4),\n",
    "        # torchvision.transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Loading TrainSet\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "# Splitting Trainset to Train / Val\n",
    "train_set_size = int(len(trainset) * 0.8)\n",
    "valid_set_size = len(trainset) - train_set_size\n",
    "\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "trainset, validset = torch.utils.data.random_split(trainset, [train_set_size, valid_set_size], generator=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Loading\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dataloaders\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "valid_dataloader = torch.utils.data.DataLoader(validset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the model <a name=\"build\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=32)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=64)\n",
    "\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=64)\n",
    "\n",
    "        self.l1 = nn.Sequential(nn.Linear(64 * 8 * 8, 128), nn.BatchNorm1d(128))\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.l2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)))\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)))\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "       \n",
    "        x = self.l2(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torchsummary` can help you see what the input and output in each layer is going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 32, 32]             896\n",
      "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
      "            Conv2d-3           [-1, 32, 16, 16]           9,248\n",
      "       BatchNorm2d-4           [-1, 32, 16, 16]              64\n",
      "            Conv2d-5           [-1, 64, 16, 16]          18,496\n",
      "       BatchNorm2d-6           [-1, 64, 16, 16]             128\n",
      "            Conv2d-7             [-1, 64, 8, 8]          36,928\n",
      "       BatchNorm2d-8             [-1, 64, 8, 8]             128\n",
      "            Linear-9                  [-1, 128]         524,416\n",
      "      BatchNorm1d-10                  [-1, 128]             256\n",
      "          Dropout-11                  [-1, 128]               0\n",
      "           Linear-12                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 591,914\n",
      "Trainable params: 591,914\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.94\n",
      "Params size (MB): 2.26\n",
      "Estimated Total Size (MB): 3.21\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "model = Net()\n",
    "summary(model, (3, 32, 32))  # Assuming input size of 3x32x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLitModel(L.LightningModule):\n",
    "    def __init__(self, lr = 0.0005):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        # Tip: \n",
    "        # The LightningModule allows you to automatically save all the hyperparameters passed to init simply by calling self.save_hyperparameters().\n",
    "        self.hparams.lr = lr\n",
    "        self.model = Net()\n",
    "        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=NUM_CLASSES)\n",
    "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=NUM_CLASSES)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        acc = self.train_accuracy(logits, y)\n",
    "        self.log(\"train_accuracy\", acc, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        acc = self.val_accuracy(logits, y)\n",
    "        self.log(\"val_accuracy\", acc)\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    " \n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        acc = self.val_accuracy(logits, y)\n",
    "        self.log(\"val_accuracy\", acc)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Train the model <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: log/lightning_logs\n",
      "\n",
      "  | Name           | Type               | Params\n",
      "------------------------------------------------------\n",
      "0 | model          | Net                | 591 K \n",
      "1 | train_accuracy | MulticlassAccuracy | 0     \n",
      "2 | val_accuracy   | MulticlassAccuracy | 0     \n",
      "3 | criterion      | CrossEntropyLoss   | 0     \n",
      "------------------------------------------------------\n",
      "591 K     Trainable params\n",
      "0         Non-trainable params\n",
      "591 K     Total params\n",
      "2.368     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e72879a4aa4288820bf2c9e1b2877e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jjookim/miniconda3/envs/ml_project/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "/Users/jjookim/miniconda3/envs/ml_project/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334bfca99bee480b9eb8dd8cc7e2c8dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6badfd47d8054d89b5d430540506ff99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "model = MyLitModel()\n",
    "\n",
    "# You can use the checkpoint\n",
    "\n",
    "# checkpoint = \"./log/lightning_logs/version_1/checkpoints/epoch=0-step=1250.ckpt\"\n",
    "# model = MyLitModel.load_from_checkpoint(checkpoint)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=1,\n",
    "    default_root_dir=\"./log\",\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Evaluation <a name=\"evaluate\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jjookim/miniconda3/envs/ml_project/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'test_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1602124505a4e65aa89137c2cb09839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">       val_accuracy        </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.6521000266075134     </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.9919371008872986     </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m      val_accuracy       \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.6521000266075134    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.9919371008872986    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.9919371008872986, 'val_accuracy': 0.6521000266075134}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"./log/lightning_logs/version_0/checkpoints/epoch=0-step=1250.ckpt\"\n",
    "model = MyLitModel.load_from_checkpoint(checkpoint)\n",
    "\n",
    "\n",
    "trainer.test(model, dataloaders=test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "ml_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
